{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cd0b1-90aa-4535-bb09-449e73cb64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7b71f-22f3-4a68-8d50-7b3aaaf73fe8",
   "metadata": {},
   "source": [
    "# Document Embeddings and Vector Databases\n",
    "\n",
    "A fundamental part of Retrieval Augmented Generation (RAG) based systems is the ability to search for documents that are relevant to a given query.  By retrieving these relevant documents, we can inject our prompts with further contextual information, allowing for the LLMs to provide better, grounded answers, often without the need for the LLM to be finetuned off of the data.\n",
    "\n",
    "In this notebook, we will demonstrate the concepts of how to build the semantic search system which powers the search and retrieval for RAG.  Broadly, the steps will be:\n",
    "\n",
    "1. Vector Embeddings Conceptually\n",
    "    - Turning Documents into Vector Embeddings via Neural Networks\n",
    "    - Retrieving Relevant Documents via Vector Similarity\n",
    "    - Simple Vector Based Retrieval System\n",
    "2. More Robust Solution using LangChain and PGVector\n",
    "    - Creating a Vector Embedding Database using LangChain and PGVector\n",
    "    - Query the Embedding Database using LangChain and PGVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72e2ea-4066-41f3-ad65-8cffe5cbbb45",
   "metadata": {},
   "source": [
    "## 1. Vector Embeddings Conceptually\n",
    "\n",
    "Document retrieval and search systems can be built off of simple heuristic concepts such as the occurence of common words, word counts, dictionaries of synonyms, etc.  Depending on the problem, this can often work fine, however for natural language, this is generally insufficient because of the semantic meaning of words, where the literal words do not carry the meaning or the concept.  As an example, the words \"pencil\" and \"eraser\" are conceptually similar to each other, both relevant to writing, however in a naive literal search system, there is almost nothing linking the two without additional context.  This problem has lead to the development of semantic search [https://en.wikipedia.org/wiki/Semantic_search], which aims to build search systems for natural language capturing \"semantic\" meaning of words.\n",
    "\n",
    "\n",
    "One of the best solutions to pop up for capturing semantics is **Vector Embeddings**, which is the idea that any type of data can be mapped into a vector space, via some embedding function, where in the new space, similar data points are \"close\" to each other.  The problem then becomes, how do you learn this embedding function?  Without diving into too much detail, it turns out that Neural Networks and Deep Learning are able to learn pretty good embedding functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc40c8-bfac-45ca-a3fe-01d4a21873ba",
   "metadata": {},
   "source": [
    "## Turning Documents into Vector Embeddings via Neural Networks\n",
    "\n",
    "With the concept in mind that **Vector Embeddings** are the process of mapping text to a vector space, let's see how this looks like in practice.  For this, our embedding function will be a Transformer like model called `sBAAI/bge-large-en-v1.5` which has been trained for this purpose of capturing semantic meaning.  More information on how many different open source embeddings models perform can be found at https://huggingface.co/spaces/mteb/leaderboard.\n",
    "\n",
    "Furthermore, in this vector space, similar concepts should be mapped close to each other as measured by cosine-similarity (used for this specific model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a54f3-d587-4888-9181-db8f6d8ee10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pprint\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f61a53-351b-409d-95f3-180bebc40a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our sample documents to turn into Vector Embeddings\n",
    "sample_docs = [\n",
    "    \"Pigs are stout-bodied, short-legged, omnivorous mammals, with thick skin usually sparsely coated with short bristles\",\n",
    "    \"Cows are four-footed and have a large body. It has two horns, two eyes plus two ears and one nose and a mouth. Cows are herbivorous animals.\",\n",
    "    \"Chickens are average-sized fowls, characterized by smaller heads, short beaks and wings, and a round body perched on featherless legs.\",\n",
    "    \"NumPy (Numerical Python) is an open source Python library that's used in almost every field of science and engineering. It's the universal standard for working with numerical data in Python, and it's at the core of the scientific Python and PyData ecosystems.\"\n",
    "]\n",
    "\n",
    "# The Embedding function, which is a Neural Network taking in text and outputting vectors\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs = {'device': 'cuda'},\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c6570-2ce5-4fc1-ba05-c23a85759835",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embedding_function.embed_documents(texts = sample_docs))\n",
    "for i, embedding in enumerate(embeddings[:1]):\n",
    "    print(f\"First 5 dimensions for embedding of {sample_docs[i]}:\")\n",
    "    print(f\"\\t {embeddings[i,:5]}\") # Only printing the first 5 to shorten it\n",
    "    print(f\"Embedding Dimension: {embeddings[i].shape}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd3daa7-6c7e-4479-9bdb-183c78fc345a",
   "metadata": {},
   "source": [
    "## Retrieving Relevant Documents via Vector Similarity\n",
    "\n",
    "A key property of these embeddings is that in the vector space, two semantically similar vectors should be close, while non-similar concepts should be far or 0.  The metric used for measuring often depends on how the model is trained.  For this model, we will use cosine similarity.\n",
    "\n",
    "With the embeddings we can compute how similar any two documents are by computing the cosine similarity between their vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d956038-2da4-4334-b8cd-8844f03a4ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(embeddings, axis = 1)\n",
    "cosine_similarities = (embeddings @ embeddings.T) / (norms.T * norms)\n",
    "for i in range(len(sample_docs)):\n",
    "    for j in range(i):\n",
    "        print(f\"Similarity between {sample_docs[j][:20]}... and {sample_docs[i][:20]}...: {cosine_similarities[j][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824cc72-f9a4-4096-9c40-7a17d2ef2202",
   "metadata": {},
   "source": [
    "As a simple eye test, we can see here that the first three documents have high cosine similarities, while each of their relationship with the description of numpy is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c685b-1678-45b7-9ed3-708fc2140601",
   "metadata": {},
   "source": [
    "## Simplest Vector Search System\n",
    "\n",
    "With only these tools, we have enough to technically build a semantic search system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a968c8-9c61-4b2b-85b1-d7e15132b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs = {'device': 'cuda'},\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    ")\n",
    "sample_docs = [\n",
    "    \"Pigs are stout-bodied, short-legged, omnivorous mammals, with thick skin usually sparsely coated with short bristles\",\n",
    "    \"Cows are four-footed and have a large body. It has two horns, two eyes plus two ears and one nose and a mouth. Cows are herbivorous animals.\",\n",
    "    \"Chickens are average-sized fowls, characterized by smaller heads, short beaks and wings, and a round body perched on featherless legs.\",\n",
    "    \"NumPy (Numerical Python) is an open source Python library that's used in almost every field of science and engineering. It's the universal standard for working with numerical data in Python, and it's at the core of the scientific Python and PyData ecosystems.\"\n",
    "]\n",
    "\n",
    "def embed_documents(docs: List[str]) -> np.ndarray:\n",
    "    \"\"\"embed all of our documents, only done once\"\"\"\n",
    "    return np.array(embedding_function.embed_documents(docs))\n",
    "\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    \"\"\"embed the query, done on demand\"\"\"\n",
    "    return np.array(embedding_function.embed_documents([query]))[0,:]\n",
    "\n",
    "def retrieve_relevant_documents(doc_embeddings : np.ndarray, query_embedding : np.ndarray, k : int = 1) -> List[Dict[str, float]]:\n",
    "    \"\"\"compute cosine similarity between query and documents, return top k and their scores\"\"\"\n",
    "    cosine_similarities = (doc_embeddings @ query_embedding) / (np.linalg.norm(doc_embeddings, axis = 1).T * np.linalg.norm(query_embedding))\n",
    "    sim_scores = np.argsort(cosine_similarities)\n",
    "    return [{'document' : sample_docs[i], 'score' : cosine_similarities[i]} for i in sim_scores[::-1][:k]]\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e403c4-e91c-4a11-944e-910b92b534c3",
   "metadata": {},
   "source": [
    "First we embed our documents, typically done offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122ca49-acaf-4ab9-b6b9-0e189fd15b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = embed_documents(sample_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb50071-2988-420d-aad4-35bc1cd4c292",
   "metadata": {},
   "source": [
    "Then for every query:\n",
    "1. embed the query\n",
    "2. compute the similarity score between the query and the documents\n",
    "3. return the top k most similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982d063-8056-4532-b3a1-3c3dc4127930",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embed_query(\"What is a hog?\")\n",
    "\n",
    "relevant_docs = retrieve_relevant_documents(doc_embeddings, query_embedding, k = 2)\n",
    "pprint.pprint(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31cd478-7683-4c35-a3bb-fc40446e6b13",
   "metadata": {},
   "source": [
    "## 2. More Robust Solution using LangChain and PGVector\n",
    "\n",
    "We built our very simple retrieval system, but in practice, there are better solutions for building production ready, scalable solutions.  Primarily, when we computed our vectors, we stored them as a simple numpy array and kept it in memory.  When computing distances, we calculated the cosine similarity against every document.  However, if we had millions of documents, this solution would no longer be sufficient, whether for memory or latency constraints.  \n",
    "\n",
    "In this section, we will demonstrate the use of PGVector and LangChain to improve this, noting that this is one of the solutions available but not fully production grade.  \n",
    "\n",
    "`PGVector` is an extension for `postgresql` which allows for the storage of vector embeddings.  `LangChain` is a tool for working with LLM models, including building embeddings for storage withing a PGVector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2921203-2ebc-40f9-ac2f-3c6f5f8ffbc4",
   "metadata": {},
   "source": [
    "### Creating a Vector Embedding Database using LangChain and PGVector\n",
    "\n",
    "First, we will connect LangChain to PGVector so that we can have a consistent pipeline that computes embeddings for text and stores the embeddings into the PGVector Vector Database.  This will enable faster query times and persistence of our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10bde71-e870-468d-a77b-3f4d0d925c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "import glob\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b0617-4424-49ec-818e-4c045cd3c216",
   "metadata": {},
   "source": [
    "We create a connection to the database and use langchain to initialize the database from a set of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0507372-7bb0-4d4a-b46f-a0fb1b553d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The connection to the database\n",
    "CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "    driver= \"psycopg2\",\n",
    "    host = \"localhost\",\n",
    "    port = \"5432\",\n",
    "    database = \"postgres\",\n",
    "    user= \"username\",\n",
    "    password=\"password\"\n",
    ")\n",
    "\n",
    "# The embedding function that will be used to store into the database\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs = {'device': 'cuda'},\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689bb3f-923f-4574-93f3-d7fb572fd7a6",
   "metadata": {},
   "source": [
    "For larger documents, a good practice is to chunk the document into smaller segments to allow for the search to be more precise.  LangChain contains multiple methods to chunk documents into smaller parts.  Below we implement a method to load a text document from a path and chunk it into blocks for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0bd9d-9caa-4527-b02c-b3fbdf78af82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chunk_document(doc_path: str) -> List[Document]:\n",
    "    \"\"\"Chunk a document into smaller langchain Documents for embedding.\n",
    "\n",
    "    :param doc_path: path to document\n",
    "    :type doc_path: str\n",
    "    :return: List of Document chunks\n",
    "    :rtype: List[Document]\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(doc_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # split document based on the `\\n\\n` character, quite unintuitive\n",
    "    # https://stackoverflow.com/questions/76633836/what-does-langchain-charactertextsplitters-chunk-size-param-even-do\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    \n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# load the document and split it into chunks\n",
    "doc_chunks = []\n",
    "for doc in glob.glob(\"docs/*.pdf\"):\n",
    "    doc_chunks += chunk_document(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb3e87-11b3-46c1-af98-4eee95693fe9",
   "metadata": {},
   "source": [
    "Now we can initilize and populate our vector database with these chunks.  Behind the scenes, each text is embedded using the `embedding_function` above, which is the `sentence-transformers/all-MiniLM-L6-v2` neural network model, and then a sql command is run to store the embedding into `postgresql`.  See the Appendix below to look at the precise schema that is created and managed by LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742f6a6-c595-4a93-8f7d-d797c9e5c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = PGVector.from_documents(\n",
    "    doc_chunks[:5],\n",
    "    connection_string = CONNECTION_STRING,\n",
    "    collection_name = \"embeddings\",\n",
    "    embedding = embedding_function,\n",
    "    pre_delete_collection = True, # uncomment this to delete existing database first\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50367d-8658-4b82-bc06-d097efbf79ce",
   "metadata": {},
   "source": [
    "If we want to add new document embeddings, we can do so as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35391d71-7d63-454e-b2db-7ce327cb025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc_ids = db.add_documents(doc_chunks[5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be588ac-741a-4767-bedf-b4a7f2cba970",
   "metadata": {},
   "source": [
    "## Query the Embedding Database using LangChain and PGVector\n",
    "\n",
    "With the Vector DB built out and populated, we can now query it using LangChain and PGVector.  We replicate some code below to demonstrate that this process can exist without the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4991bb-c0bc-44b9-9c74-d0d59cbc43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08190ba5-fb54-4e6c-8d78-711703b4cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The connection to the database\n",
    "CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "    driver= \"psycopg2\",\n",
    "    host = \"localhost\",\n",
    "    port = \"5432\",\n",
    "    database = \"postgres\",\n",
    "    user= \"username\",\n",
    "    password=\"password\"\n",
    ")\n",
    "\n",
    "# The embedding function that will be used to store into the database\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs = {'device': 'cuda'},\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Creates the database connection to our existing DB\n",
    "db = PGVector(\n",
    "    connection_string = CONNECTION_STRING,\n",
    "    collection_name = \"embeddings\",\n",
    "    embedding_function = embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff0c3f-5ac8-4a4e-b406-d7548568b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query it, note that the score here is a distance metric (lower is more related)\n",
    "query = \"What's the efficacy of NeuroGlyde?\"\n",
    "docs_with_scores = db.similarity_search_with_score(query, k = 3)\n",
    "\n",
    "# print results\n",
    "for doc, score in docs_with_scores:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3253c-6c01-41e7-b0a8-2b38f3a894c4",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## LangChain Documentation for PGVector\n",
    "\n",
    "https://python.langchain.com/docs/integrations/vectorstores/pgvector\n",
    "\n",
    "https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.pgvector.PGVector.html\n",
    "\n",
    "## LangChain DB Schema for Embeddings\n",
    "\n",
    "### Table Schema of Collection\n",
    "\n",
    "```\n",
    "Table \"public.langchain_pg_collection\"\n",
    "  Column   |       Type        | Collation | Nullable | Default \n",
    "-----------+-------------------+-----------+----------+---------\n",
    " name      | character varying |           |          | \n",
    " cmetadata | json              |           |          | \n",
    " uuid      | uuid              |           | not null | \n",
    "Indexes:\n",
    "    \"langchain_pg_collection_pkey\" PRIMARY KEY, btree (uuid)\n",
    "Referenced by:\n",
    "    TABLE \"langchain_pg_embedding\" CONSTRAINT \"langchain_pg_embedding_collection_id_fkey\" FOREIGN KEY (collection_id) REFERENCES langchain_pg_collection(uuid) ON DELETE CASCADE\n",
    "```\n",
    "\n",
    "### Table Schema of Embeddings\n",
    "\n",
    "```\n",
    "              Table \"public.langchain_pg_embedding\"\n",
    "    Column     |       Type        | Collation | Nullable | Default \n",
    "---------------+-------------------+-----------+----------+---------\n",
    " collection_id | uuid              |           |          | \n",
    " embedding     | vector            |           |          | \n",
    " document      | character varying |           |          | \n",
    " cmetadata     | json              |           |          | \n",
    " custom_id     | character varying |           |          | \n",
    " uuid          | uuid              |           | not null | \n",
    "Indexes:\n",
    "    \"langchain_pg_embedding_pkey\" PRIMARY KEY, btree (uuid)\n",
    "Foreign-key constraints:\n",
    "    \"langchain_pg_embedding_collection_id_fkey\" FOREIGN KEY (collection_id) REFERENCES langchain_pg_collection(uuid) ON DELETE CASCADE\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
