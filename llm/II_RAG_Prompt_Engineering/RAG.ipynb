{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f7b71f-22f3-4a68-8d50-7b3aaaf73fe8",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) Using our Vector DB\n",
    "\n",
    "In section I, we built a Vector DB to allow for retrieval of similar documents.  This direct followup will show how to use the Vector DB to enhance our prompts with additional context before we put it into a Large Language Model.  \n",
    "\n",
    "The notebook follows as:\n",
    "\n",
    "1. RAG Conceptually\n",
    "   - Question-Answering using Large Language Models\n",
    "   - Retrieval of Relevant Documents for a Query\n",
    "   - Question-Answering using RAG for Document Context\n",
    "2. Using built-in LangChain RAG prompts and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72e2ea-4066-41f3-ad65-8cffe5cbbb45",
   "metadata": {},
   "source": [
    "## 1. RAG Conceptually\n",
    "\n",
    "Large Language Models have proven to be very good at general question and answering tasks.  However, a main limitation of many LLMs is that they are generally constrained to the data that they are initially trained on.  Without access to an external data source, LLMs cannot bring in new information, whether this is proprietary domain specific knowledge or just an update on an existing knowledge base.  Given that, how can we enable LLMs to be updated with new information while leveraging the powerful language properties?\n",
    "\n",
    "One solution to this Retrieval Augumented Generation (RAG).  In RAG, we leverage the fact that LLMs can be prompted with additional context data to add additional relevant context to a given query before we pass it into the model.  The old pipeline would be:\n",
    "\n",
    "```\n",
    "Query ------> LLM\n",
    "```\n",
    "\n",
    "which with RAG will be updated to\n",
    "\n",
    "```\n",
    "Query ------> Retrieve Relevant Documents ------> Augmented Query ------> LLM\n",
    "```\n",
    "\n",
    "We will retrieve relevant documents using the knowledge base we built with the Vector DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd74da-3a1a-4d2d-98a0-1e2cd14d45d6",
   "metadata": {},
   "source": [
    "### Question-Answering using Large Language Models\n",
    "\n",
    "We start by looking at a question answering system that simply asks the LLM a question.  In this case, if the model doesn't already know the answer, then there's not much way to inject that knowledge into the model.  Some models may immediately identify that there's not enough context while other models may go off rails and hallucinate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba12d4-0b6b-4632-886a-22f7b6619698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee6de56-c4d6-49d6-8de3-dae631370833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FIRST TIME YOU RUN THIS, IT MIGHT TAKE A WHILE\n",
    "\n",
    "model_path_or_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    use_flash_attention_2=True,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "def generate(prompt):\n",
    "    \"\"\"Convenience function for generating model output\"\"\"\n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True).input_ids.cuda()\n",
    "    \n",
    "    # Generate new tokens based on the prompt, up to max_new_tokens\n",
    "    # Sample aacording to the parameter\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, \n",
    "            max_new_tokens=100, \n",
    "            do_sample=True, \n",
    "            top_p=0.9,\n",
    "            temperature=0.9,\n",
    "            use_cache=True\n",
    "        )\n",
    "    return tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df20081-5efd-49f8-8a75-6985d0a553ae",
   "metadata": {},
   "source": [
    "Let's ask it a very general question because the ChatGPT has been trained on a huge amount of data and providing any specifics in the question will likely result in a correct answer.  In this situation, the model can't possibly ground itself because it doesn't know the context - yet it will still answer with something that it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6253ce-3580-42db-8eaa-8421f321738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input for for tokenization, attach any prompt that should be needed\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "    Question: {query}\n",
    "\n",
    "    Answer: \n",
    "\"\"\"\n",
    "\n",
    "query = \"What's the efficacy of NeuroGlyde?\"\n",
    "prompt = PROMPT_TEMPLATE.format(query = query)\n",
    "\n",
    "res = generate(prompt)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Generated Response:\\n{res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da026b3e-c31d-4714-bf43-50e6178b3293",
   "metadata": {},
   "source": [
    "It doesn't know the context, so let's provide it the context.  Which context should we provide?  The context will be retrieved from our vector databse.\n",
    "\n",
    "We will retrieve the relevant documents to this question, inject it into the prompt, and send that to the model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc40c8-bfac-45ca-a3fe-01d4a21873ba",
   "metadata": {},
   "source": [
    "### Retrieval of Relevant Documents for a Query\n",
    "\n",
    "We'll briefly revisit our code to retrieve documents from our previous example.  This Vector DB has already been populated with a set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a54f3-d587-4888-9181-db8f6d8ee10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db092bd-47a2-4d5f-939d-97cdaf280331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The connection to the database\n",
    "CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "    driver= \"psycopg2\",\n",
    "    host = \"localhost\",\n",
    "    port = \"5432\",\n",
    "    database = \"postgres\",\n",
    "    user= \"username\",\n",
    "    password=\"password\"\n",
    ")\n",
    "\n",
    "# The embedding function that will be used to store into the database\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs = {'device': 'cuda'},\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Creates the database connection to our existing DB\n",
    "db = PGVector(\n",
    "    connection_string = CONNECTION_STRING,\n",
    "    collection_name = \"embeddings\",\n",
    "    embedding_function = embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c6570-2ce5-4fc1-ba05-c23a85759835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query it, note that the score here is a distance metric (lower is more related)\n",
    "query = \"What's the efficacy of NeuroGlyde?\"\n",
    "docs_with_scores = db.similarity_search_with_score(query, k = 1)\n",
    "\n",
    "# print results\n",
    "for doc, score in docs_with_scores:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257a0ee-96bb-4729-8c25-5141628ad349",
   "metadata": {},
   "source": [
    "When we query, we get the most relevant document for this query.  Let's create a new prompt that can take this new context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e12fbf-d932-4e61-a382-9c02b9eb0674",
   "metadata": {},
   "source": [
    "### Question-Answering using RAG for Document Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55311ce-5281-42c7-a199-2d316be10d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input for for tokenization, attach any prompt that should be needed\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question using only this context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "query = \"What's the efficacy of NeuroGlyde?\"\n",
    "docs_with_scores = db.similarity_search_with_score(query, k = 1)\n",
    "context_prompt = RAG_PROMPT_TEMPLATE.format(\n",
    "    context = docs_with_scores[0][0].page_content,\n",
    "    query = query\n",
    ")\n",
    "\n",
    "res = generate(context_prompt)\n",
    "\n",
    "print(f\"Prompt:\\n{context_prompt}\\n\")\n",
    "print(f\"Generated Response:\\n{res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339ebf9-67f5-42f4-851d-3ef233c0d296",
   "metadata": {},
   "source": [
    "That's it! That's the general concept of Retrieval Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c1977-6931-4399-bb03-51b39d4a16d8",
   "metadata": {},
   "source": [
    "## Using built in LangChain RAG chains instead\n",
    "\n",
    "LangChain contains many built-in methods that have connectivity to Vector Databases and LLMs.  In the example above, we built a custom prompt template and manually retrieved the document, then put it into the chain.  While pretty simple, with LangChain, this can all be pipelined together and more can be done, such as retrieving meta-data and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeba65f-56b7-49a0-a5f4-7e217aa70173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# Turn our db into a retriever\n",
    "retriever = db.as_retriever(search_kwargs = {'k' : 2})\n",
    "\n",
    "# Turn our model into an LLM\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=100)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question using only this context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \n",
    "\"\"\")                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8dbc34-26a8-4719-befe-747dc4e95510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build a chain with multiple documents for RAG\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 2-step chain, first retrieve documents\n",
    "# Then take those documents and store relevant infomration in `document_sources`\n",
    "# Pass the prompt into the document chain\n",
    "rag_chain_with_source = RunnableParallel({\n",
    "    \"documents\": retriever, \n",
    "     \"question\": RunnablePassthrough()\n",
    "}) | {\n",
    "    \"sources\": lambda input: [(doc.page_content, doc.metadata) for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1d7ba-0058-4eed-a21b-9f18f3a6b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rag_chain_with_source.invoke(\"What's the efficacy of Pentatryponal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f8327-9b13-4ee1-bb1b-9dd84eddb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399f80f-7c00-4496-90e8-7d8d05583de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['sources']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
