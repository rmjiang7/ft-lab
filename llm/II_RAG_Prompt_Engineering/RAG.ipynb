{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f7b71f-22f3-4a68-8d50-7b3aaaf73fe8",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) Using our Vector DB\n",
    "\n",
    "In section I, we built a Vector DB to allow for retrieval of similar documents.  This direct followup will show how to use the Vector DB to enhance our prompts with additional context before we put it into a Large Language Model.  \n",
    "\n",
    "The notebook follows as:\n",
    "\n",
    "1. RAG Conceptually\n",
    "   - Question-Answering using Large Language Models\n",
    "   - Retrieval of Relevant Documents for a Query\n",
    "   - Question-Answering using RAG for Document Context\n",
    "2. Using built-in LangChain RAG prompts and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72e2ea-4066-41f3-ad65-8cffe5cbbb45",
   "metadata": {},
   "source": [
    "## 1. RAG Conceptually\n",
    "\n",
    "Large Language Models have proven to be very good at general question and answering tasks.  However, a main limitation of many LLMs is that they are generally constrained to the data that they are initially trained on.  Without access to an external data source, LLMs cannot bring in new information, whether this is proprietary domain specific knowledge or just an update on an existing knowledge base.  Given that, how can we enable LLMs to be updated with new information while leveraging the powerful language properties?\n",
    "\n",
    "One solution to this Retrieval Augumented Generation (RAG).  In RAG, we leverage the fact that LLMs can be prompted with additional context data to add additional relevant context to a given query before we pass it into the model.  The old pipeline would be:\n",
    "\n",
    "```\n",
    "Query ------> LLM\n",
    "```\n",
    "\n",
    "which with RAG will be updated to\n",
    "\n",
    "```\n",
    "Query ------> Retrieve Relevant Documents ------> Augmented Query ------> LLM\n",
    "```\n",
    "\n",
    "We will retrieve relevant documents using the knowledge base we built with the Vector DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd74da-3a1a-4d2d-98a0-1e2cd14d45d6",
   "metadata": {},
   "source": [
    "### Question-Answering using Large Language Models\n",
    "\n",
    "We start by looking at a question answering system that simply asks the LLM a question.  In this case, if the model doesn't already know the answer, then there's not much way to inject that knowledge into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94ce4c57-d3d0-4fb3-80da-9a4fe2eb69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(openai_api_key=\"\")\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{query}\"\n",
    ")\n",
    "chain = LLMChain(prompt=prompt_template, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df20081-5efd-49f8-8a75-6985d0a553ae",
   "metadata": {},
   "source": [
    "Let's ask it a very general question because the ChatGPT has been trained on a huge amount of data and providing any specifics in the question will likely result in a correct answer.  In this situation, the model can't possibly ground itself because it doesn't know the context - yet it will still answer with something that it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d6253ce-3580-42db-8eaa-8421f321738d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nThe Wine Keeper's Shop is an online store specializing in wine storage solutions. They offer a variety of products including wine racks, wine coolers, cellars, and other accessories. They also provide expert advice and assistance in selecting the best wine storage solution for your needs.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(query = \"What is the wine keeper's shop?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da026b3e-c31d-4714-bf43-50e6178b3293",
   "metadata": {},
   "source": [
    "It doesn't know the context, so let's provide it the context.  Which context should we provide?  The context will be retrieved from our vector databse.\n",
    "\n",
    "We will retrieve the relevant documents to this question, inject it into the prompt, and send that to the model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc40c8-bfac-45ca-a3fe-01d4a21873ba",
   "metadata": {},
   "source": [
    "### Retrieval of Relevant Documents for a Query\n",
    "\n",
    "We'll briefly revisit our code to retrieve documents from our previous example.  This Vector DB has already been populated with a set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "077a54f3-d587-4888-9181-db8f6d8ee10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2db092bd-47a2-4d5f-939d-97cdaf280331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The connection to the database\n",
    "CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "    driver= \"psycopg2\",\n",
    "    host = \"localhost\",\n",
    "    port = \"5432\",\n",
    "    database = \"postgres\",\n",
    "    user= \"username\",\n",
    "    password=\"password\"\n",
    ")\n",
    "\n",
    "# The embedding function that will be used to store into the database\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Creates the database connection to our existing DB\n",
    "db = PGVector(\n",
    "    connection_string = CONNECTION_STRING,\n",
    "    collection_name = \"embeddings\",\n",
    "    embedding_function = embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f94c6570-2ce5-4fc1-ba05-c23a85759835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.44882341914316437\n",
      "The wine-shop keeper accordingly rolled his eyes about, until they\n",
      "rested upon an elderly gentleman and a young lady, who were seated in\n",
      "a corner. Other company were there: two playing cards, two playing\n",
      "dominoes, three standing by the counter lengthening out a short supply\n",
      "of wine. As he passed behind the counter, he took notice that the\n",
      "elderly gentleman said in a look to the young lady, “This is our man.”\n",
      "\n",
      "“What the devil do _you_ do in that galley there?” said Monsieur Defarge\n",
      "to himself; “I don’t know you.”\n",
      "\n",
      "But, he feigned not to notice the two strangers, and fell into discourse\n",
      "with the triumvirate of customers who were drinking at the counter.\n",
      "\n",
      "“How goes it, Jacques?” said one of these three to Monsieur Defarge. “Is\n",
      "all the spilt wine swallowed?”\n",
      "\n",
      "“Every drop, Jacques,” answered Monsieur Defarge.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# query it, note that the score here is a distance metric (lower is more related)\n",
    "query = \"What is the wine keeper's shop?\"\n",
    "docs_with_scores = db.similarity_search_with_score(query, k = 1)\n",
    "\n",
    "# print results\n",
    "for doc, score in docs_with_scores:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257a0ee-96bb-4729-8c25-5141628ad349",
   "metadata": {},
   "source": [
    "When we query, we get the most relevant document for this query.  Let's create a new prompt that can take this new context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e12fbf-d932-4e61-a382-9c02b9eb0674",
   "metadata": {},
   "source": [
    "### Question-Answering using RAG for Document Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a55311ce-5281-42c7-a199-2d316be10d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_context = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question using only this context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\")\n",
    "rag_chain = LLMChain(prompt=rag_prompt_context, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "600034a7-68f9-4c31-9255-9c7735cd10ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe wine-keeper's shop appears to be a tavern where people were drinking wine and playing cards and dominoes.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the wine keeper's shop?\"\n",
    "docs_with_scores = db.similarity_search_with_score(query, k = 1)\n",
    "\n",
    "rag_chain.run(\n",
    "    context = docs_with_scores[0][0],\n",
    "    query = query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339ebf9-67f5-42f4-851d-3ef233c0d296",
   "metadata": {},
   "source": [
    "That's it! That's the general concept of Retrieval Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c1977-6931-4399-bb03-51b39d4a16d8",
   "metadata": {},
   "source": [
    "## Using built in LangChain RAG chains\n",
    "\n",
    "LangChain contains many built-in methods that have connectivity to Vector Databases and LLMs.  In the example above, we built a custom prompt template and manually retrieved the document, then put it into the chain.  While pretty simple, with LangChain, this can all be pipelined together and more can be done, such as retrieving meta-data and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1eeba65f-56b7-49a0-a5f4-7e217aa70173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from operator import itemgetter\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs = {'k' : 2})\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d8dbc34-26a8-4719-befe-747dc4e95510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build a chain with multiple documents for RAG\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 2-step chain, first retrieve documents\n",
    "# Then take those documents and store relevant infomration in `document_sources`\n",
    "# Pass the prompt into the document chain\n",
    "rag_chain_with_source = RunnableParallel({\n",
    "    \"documents\": retriever, \n",
    "     \"question\": RunnablePassthrough()\n",
    "}) | {\n",
    "    \"sources\": lambda input: [(doc.page_content, doc.metadata) for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d9c1d7ba-0058-4eed-a21b-9f18f3a6b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rag_chain_with_source.invoke(\"What is the wine keeper's shop?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd4f7660-bafc-4247-b00c-a3aa8ae824fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The wine-keeper's shop is a corner shop, better than most others in appearance and degree, owned and operated by Monsieur Defarge. Customers visit the shop to purchase wine and other items, and the shopkeeper also provides conversation and other services.\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2381672a-e56b-4570-a184-81bba1d30ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The wine-shop keeper accordingly rolled his eyes about, until they\\nrested upon an elderly gentleman and a young lady, who were seated in\\na corner. Other company were there: two playing cards, two playing\\ndominoes, three standing by the counter lengthening out a short supply\\nof wine. As he passed behind the counter, he took notice that the\\nelderly gentleman said in a look to the young lady, “This is our man.”\\n\\n“What the devil do _you_ do in that galley there?” said Monsieur Defarge\\nto himself; “I don’t know you.”\\n\\nBut, he feigned not to notice the two strangers, and fell into discourse\\nwith the triumvirate of customers who were drinking at the counter.\\n\\n“How goes it, Jacques?” said one of these three to Monsieur Defarge. “Is\\nall the spilt wine swallowed?”\\n\\n“Every drop, Jacques,” answered Monsieur Defarge.',\n",
       "  {'source': 'pg98.txt'}),\n",
       " ('For, the time was to come, when the gaunt scarecrows of that region\\nshould have watched the lamplighter, in their idleness and hunger, so\\nlong, as to conceive the idea of improving on his method, and hauling\\nup men by those ropes and pulleys, to flare upon the darkness of their\\ncondition. But, the time was not come yet; and every wind that blew over\\nFrance shook the rags of the scarecrows in vain, for the birds, fine of\\nsong and feather, took no warning.\\n\\nThe wine-shop was a corner shop, better than most others in its\\nappearance and degree, and the master of the wine-shop had stood outside\\nit, in a yellow waistcoat and green breeches, looking on at the struggle\\nfor the lost wine. “It’s not my affair,” said he, with a final shrug\\nof the shoulders. “The people from the market did it. Let them bring\\nanother.”\\n\\nThere, his eyes happening to catch the tall joker writing up his joke,\\nhe called to him across the way:\\n\\n“Say, then, my Gaspard, what do you do there?”',\n",
       "  {'source': 'pg98.txt'})]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['sources']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d8529-ef9c-41af-a805-873d704627db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
