{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Finetuning LLMs with QLoRA\n",
    "\n",
    "The process in this notebook follows from: https://www.philschmid.de/instruction-tune-llama-2\n",
    "\n",
    "In this lab, we will demonstrate finetuning a Large Language Model to better respond to instructions.  This is frequently referred to as Instruction Fine-Tuning. \n",
    "\n",
    "- [Preparing the Dataset](#preparing-the-dataset)\n",
    "- [Selecting the Base Pre-trained Model](#selecting-the-base-pre-trained-model)\n",
    "- [Finetuning the Model](#finetuning-the-model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "Fine-tuning LLMs is primarily used for teaching the model new behavior, such as better responding to instructions, responding with certain tones, or acting more as a conversational chatbot.  \n",
    "\n",
    "The dataset for finetuning LLMs are text entries formatted in the way ***THAT WE WISH FOR AN INTERACTION WITH THE MODEL TO LOOK LIKE***.  For example, if we wish for the model to follow instructions better, we should provide a dataset which gives examples of it following instructions.  **This is almost exactly like few-shot prompting, but reinforcing the behavior even further by actually changing the weights of the model.**  After fine-tuning, few-shot prompts will not be necessary.\n",
    "\n",
    "\n",
    "### Dataset using `datasets`\n",
    "\n",
    "The dataset that we will be using for instruction fine-tuning is a dataset hand-curated by databricks for instruction following called \"dolly-15k\".  Quality data is key for getting finetuning to work successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Download the dataset from HuggingFace\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split = \"train\")\n",
    "# dataset = dataset.select(range(100)) # limit the size for testing\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base dataset contains columns for an `instruction`, an optional `context`, and an `response` that we want the bot to respond to.  However, to feed it into the model for finetuning, we need to combine each column so that 1 row corresponds to 1 example interaction with the model.  \n",
    "\n",
    "This 1 entry should be an example to the LLM about:\n",
    "\n",
    "1. How we wish to interact with the model (prompt)\n",
    "2. How we want the model to respond\n",
    "\n",
    "Remember, these generative LLMs are trained to read in a provided prompt, and essentially auto-complete the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    # This function takes a row of the above dataset and returns a single text string\n",
    "    # with an example interaction\n",
    "\n",
    "\treturn f\"\"\"### Instruction:\n",
    "    Use the following Input and come up with a structured response.\n",
    "\t\n",
    "    ### Input:\n",
    "    {sample['instruction']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['response']}\n",
    "    \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of instruction following, the example interaction from the above function will look like:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "Use the following Input and come up with a structured response.\n",
    "\n",
    "### Input:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\n",
    "```\n",
    "\n",
    "At inference time, we will pass in the top part of our prompt, with our instruction,\n",
    "```\n",
    "### Instruction:\n",
    "Use the following Input and come up with a structured response.\n",
    "\n",
    "### Input:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "and let the model autocomplete to fill in the `{response}`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Base Pre-trained Model\n",
    "\n",
    "Once we have the data, we can select the base model that we would like to fine tune for this behavior.  \n",
    "\n",
    "The model that we will select is the `Llama-2-7b` base model.  This is a 7b parameter model, quite small in the grand scheme of LLMs, but one that produces good quality results, especially compared to models of similar sizes.  Note that we fine-tune the base version of the model, not the chat fine-tuned model.  For introducing new behavior, it is generally better to fine tune the base version of the model rather than one that is already fine-tuned for another task.\n",
    "\n",
    "### Quantization using `bitsandbytes`\n",
    "\n",
    "LLMs are extremely memory intensive.  One trick that is commonly used when working with LLMs to reduce memory usage as well as increase computational speed for both inference and training, is reducing the precision of the weights from full precision 32-bit floating points (fp32) to lower precisions such as int8, fp4, nf4, etc.  This is known as quantization.  Research has shown that quantization often times has minimal impact on the quality of generations, but this is on a case-by-case basis. \n",
    "\n",
    "In this example, we will be quantizing and fine-tuning using normal-float 4 bit (nf4).  In practice, the quantization behind the scenes is handled by the `bitsandbytes` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Hugging Face Base Model ID\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False, \n",
    "    device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model loaded up, we are ready to finetune using our dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning the Model\n",
    "\n",
    "There are two main ways to finetune a large language model:\n",
    "\n",
    "1. Pre-training/Full Finetuning\n",
    "\n",
    "    In this situation, all of the model weights (all 7b of them) are set to be trainable and tweaked during training.  This can lead to the most dramatic changes in model behavior but is also the most computationally expensive.  \n",
    "    \n",
    "    When initially training the model, also known as pre-training, this is necessarily done and where you see the extreme computational costs show up (i.e. 500 A100 80GB GPUs trained for 10000 hours, etc...).\n",
    "\n",
    "2. Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "    Parameter efficient finetuning methods are an alternative to full finetuning where, instead of training the parameters of the pre-trained model, a subset of new parameters are trained without touching the base model weights. These new trainable parameters are injected into the model mathematically at different points to change the outcome.  There are a handful of methods that use this approach such as Prompt Tuning, P-Tuning, and Low-Rank Adaptation (LoRA).  For this lab, we will focus on LoRA.  \n",
    "\n",
    "    LoRA methods introduce a set of trainable rank-decomposition matrices (update matrices) which can be used to modify the existing weights of the pre-trained model.  The typical location that these matrices are placed are within the attention layers, so they are not exclusive to LLMs.  The size of these update matrices can be controlled by  setting the desired rank of the matrix (lora_r), with smaller rank corresponding to smaller matrices and thus fewer trainable parameters.   During fine-tuning, only these update matrices are tuned and often times, this makes the total number of trainable parameters a very small fraction of the total number of weights.\n",
    "\n",
    "### Finetuning using `peft`\n",
    "\n",
    "To configure the model for paremeter efficient fine-tuning and LoRA, we will use the `peft` package.  Specifically, we will define our Lora parameters and also set to the taks to `CAUSAL_LM` to train the model for generative purposes.  Because we also quantized the model to 4-bit, we will also be using a state-of-the-art method called Quantized LoRA (QLoRA) to do this training in low precision to save memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config for QLoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# prepare model for training with low-precision\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the trainer with `trl`\n",
    "\n",
    "Now that we have prepared the data, loaded the model in 4-bit, and configured our LoRA finetuning according to our model, we are ready to train the model. Training of LLMs for generative purposes uses the causal language modeling objective.  Briefly, this specifies that when calculating attention, the model should only be able to consider things \"to the left\".  So for a sentence, it can only decide what to generate by looking at all of the words that came before it.  \n",
    "\n",
    "A very useful wrapper for training transformer based models is the Supervised Fine-Tuning Trainer (`SFTrainer`) provided by the `trl` library.  While the supervised fine tuning is typically used in the context of reinforcement learning, for our purposes, it simply refers to providing the model with examples of input, and response.  All of the actual training, including computing gradients, tweaking the optimizer, batching the data, evaluation will be done behind the scenes using the `SFTrainer` wrapper.  This will conduct the finetuning that we want after we pass in the dataset and hyperparameters.  This is much more efficient and robust than writing our own training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama-7-int4-dolly\", \n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
    ")\n",
    "\n",
    "# max sequence length for model and packing of the dataset\n",
    "max_seq_length = 2048 \n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of the configuration done, we can now run our training.  On an A10g, this takes about 3 hours to run, after which it will save the LoRA weights to the `llama-7-int4-dolly` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # there will not be a progress bar since tqdm is disabled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has finished training, it is ready to be used.  Now, hopefully, when the model sees the prompt that we crafted before, it will know how to respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
