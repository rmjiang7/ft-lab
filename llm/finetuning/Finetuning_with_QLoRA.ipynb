{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a LLM with QLoRA\n",
    "\n",
    "The process in this notebook follows from: https://www.philschmid.de/instruction-tune-llama-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "The dataset needed to train Large Language Model are text entries formatted in any way.  This includes any prompts and special tokens that we wish the model to have responses to. \n",
    "\n",
    "In this sample lab, we will demonstrate finetuning a Large Language model to respond as an instruction following bot.  The dataset that we will be using is a hand-curated dataset by databricks for instruction following called \"dolly-15k\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Download the dataset from HuggingFace\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split = \"train\")\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains columns for an `instruction`, an optional `context`, and an `response` that we want the bot to respond to.  However, to feed it into the model for finetuning, we need to combine each column so that 1 row corresponds to 1 entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    # This function takes a row of the above dataset and returns a single text string\n",
    "\n",
    "\treturn f\"\"\"### Instruction:\n",
    "    Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "    ### Input:\n",
    "    {sample['response']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['instruction']}\n",
    "    \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Pretrained Base Model & Tokenizer\n",
    "\n",
    "Once we have the data that we will be using to fine tune the model, we can select the base model that we would like to fine tune.  The model that we will select is the `Llama-2-7b` base model.  This is a 7b parameter model, quite small in the grand scheme of LLMs, but one that produces good quality results, especially compared to models of similar sizes.\n",
    "\n",
    "There is quite a bit of knowledge already embedded within the base model, so the fine-tuning we do here will largely just be to change the behavior of the model to act more like a instruction following bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Hugging Face Base Model ID\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False, \n",
    "    device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory and computational purposes, we will also be using the `bitsandbytes` library to load and fine tune the model in lower precisions.  More specifically, we will be loading the model weights in 4-bit, compared to a full precision model at 32-bit.  Lowering the precision of the weights, also called ***quantization***,  results in substantial memory and computational savings at the cost of some accuracy.  However, this is common practice now as several studies have shown that the loss in quality of generated responses can be quite minor compared to the benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BitsAndBytesConfig int-4 config\n",
    "# This is for the most memory efficient training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the LoRA Finetuning configuration\n",
    "\n",
    "There are two main ways to finetune a large language model:\n",
    "\n",
    "1. Pre-training/Full Finetuning\n",
    "\n",
    "    In this situation, all of the model weights (all 7b of them) are set to be trainable and tweaked during training.  This can lead to the most dramatic changes in model behavior but is also the most computationally expensive.  When initially training the model, also known as pre-training, this is necessarily done and where you see the extreme computational costs show up (i.e. 500 A100 80GB GPUs trained for 10000 hours, etc...).\n",
    "\n",
    "2. Parameter Efficient Fine-Tuning\n",
    "\n",
    "    Parameter efficient finetuning methods are an alternative to full finetuning where, instead of training the parameters of the pre-trained model, a subset of new parameters are trained without touching the base model weights. These new trainable parameters are injected into the model mathematically at different points to change the outcome.  There are a handful of methods that use this approach such as Prompt Tuning, P-Tuning, and Low-Rank Adaptation.  For this lab, we will focus on Low-Rank Adaptation (LoRA).  \n",
    "\n",
    "    LoRA methods introduce a set of trainable rank-decomposition matrices (update matrices) which can be used to modify the existing weights of the pre-trained model.  The typical location that these matrices are placed are within the attention layers, so they are not exclusive to LLMs.  The size of these update matrices can be controlled by the setting the desired rank of the matrix, with smaller rank corresponding to smaller matrices and thus fewer trainable parameters.   During fine-tuning, only these update matrices are tuned and often times, this makes the total number of trainable parameters a very small fraction of the total number of weights.\n",
    "\n",
    "Below we define our desired LoRA configuration using the `peft` package.  We will also be using a state-of-the-art method called Quantized LoRA (QLoRA) to also do this training in low precision to save memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config for QLoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning the Model\n",
    "\n",
    "Now that we have prepared the data, loaded the model in 4-bit, and configured our LoRA finetuning according to our model, we are ready to train the model. \n",
    "\n",
    "All of the actual training, including computing gradients, tweaking the optimizer, batching the data, measuring evaluation will be done behind the scenes using the `SFTrainer` wrapper.  This will conduct the finetuning that we want after we pass in the dataset and hyperparameters.  In practice, this is much more efficient and robust than writing our own training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama-7-int4-dolly\", \n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=6,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
    ")\n",
    "\n",
    "# max sequence length for model and packing of the dataset\n",
    "max_seq_length = 2048 \n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Supervisted Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "trainer.save_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has finished training, it is ready to be used.  Now, hopefully, when the model sees the prompt that we crafted before, it will know how to respond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
