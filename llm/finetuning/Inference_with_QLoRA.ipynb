{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a QLoRA Finetuned Model For Inference\n",
    "\n",
    "In the previous notebook, we learned how to finetune a model using QLoRA, which output a directory with the LoRA weights.  This is often times much smaller than the full model with the idea that the adapter can be saved locally while the base model can live somewhere else.  On demand, the base model can be downloaded, and the adapter applied to the base model to get the fine-tuned model.\n",
    "\n",
    "This composable based system is very handy for keeping storage low and quickly switching to new adapters on demand.\n",
    "\n",
    "### Loading the model using `peft`\n",
    "\n",
    "A QLoRA adapter can be loaded along with the base model by using the `AutoPeftModelForCausalLM` class and passing in the path to your fine-tuned LoRA weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path = \"path-to-lora-adapter\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    lora_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation using the Finetuned Model\n",
    "\n",
    "In the previous example, we fine-tuned the model to respond to a specific prompt format.  \n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "Use the following Input and come up with a structured response.\n",
    "\n",
    "### Input:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "\n",
    "The idea of finetuning was that, when we pass in an instruction following this format, the fine-tuned model will know how to auto-complete it with an appropriate response.  The following function will automatically format the prompt, given an instruction, to feed into the model, expecting the completion results (or generation) after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(instruction):\n",
    "    # Convenience function to format our prompt correctly for the bot\n",
    "    return  f\"\"\"### Instruction:\n",
    "    Use the following Input and come up with a structured response.\n",
    "    \n",
    "    ### Input:\n",
    "    {instruction}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate new responses, we will\n",
    "\n",
    "1. Format the instruction into the fine-tuned format\n",
    "2. Tokenize the input using the models tokenizer\n",
    "3. Generate Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Tell me the phases of the moon.\"\n",
    "prompted_instruction = format_prompt(instruction)\n",
    "input_ids = tokenizer(\n",
    "    prompted_instruction,\n",
    "    return_tensors=\"pt\", \n",
    "    truncation=True).input_ids.cuda()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "\n",
    "print(f\"Prompt:\\n{instruction}\\n\")\n",
    "print(f\"Generated Response:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompted_instruction):]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
