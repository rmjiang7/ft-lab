{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a QLoRA Finetuned Model For Inference\n",
    "\n",
    "A QLoRA adapter can be loaded along with the base model by using the `AutoPeftModelForCausalLM` class.  By loading the model in this way, it can be used as in any other HuggingFace LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path = \"llama-7-int4-dolly\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    lora_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation using the Finetuned Model\n",
    "\n",
    "Often time, a finetuned model expects a specific prompt format in order to behave in the way that it was trained for.  \n",
    "\n",
    "In this example, we used the instruction finetuning example format with `### Input:` and `### Response:` tags, where our instruction with follow the input tag, and we want the model to respond after the response tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(instruction):\n",
    "    # Convenience function to format our prompt correctly for the bot\n",
    "    return  f\"\"\"### Instruction:\n",
    "    Use the following Input and come up with a structured response.\n",
    "    \n",
    "    ### Input:\n",
    "    {instruction}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed in an instruction to our model, we will:\n",
    "\n",
    "1. Format the instruction in the expected prompt format\n",
    "2. Encode the prompt into tokens using the tokenizer\n",
    "3. Call `generate` with the tokenized prompt and pass in any generation parameters we want\n",
    "4. Decode the output using the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Red, Green, Blue\"\n",
    "prompted_instruction = format_prompt(instruction)\n",
    "input_ids = tokenizer(\n",
    "    prompted_instruction,\n",
    "    return_tensors=\"pt\", \n",
    "    truncation=True).input_ids.cuda()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "\n",
    "print(f\"Prompt:\\n{instruction}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompted_instruction):]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
